{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/msmarcotop100/msmarco-doctrain-top100\n/kaggle/input/msmarco-docs-data/msmarco-docs.tsv\n/kaggle/input/ms-marco-queries/msmarco-doctrain-queries.tsv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install fast-autocomplete\n","execution_count":3,"outputs":[{"output_type":"stream","text":"Collecting fast-autocomplete\n  Downloading fast_autocomplete-0.6.0-py3-none-any.whl (22 kB)\nRequirement already satisfied: python-Levenshtein>=0.12.0 in /opt/conda/lib/python3.7/site-packages (from fast-autocomplete) (0.12.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from python-Levenshtein>=0.12.0->fast-autocomplete) (46.1.3.post20200325)\nInstalling collected packages: fast-autocomplete\nSuccessfully installed fast-autocomplete-0.6.0\n\u001b[33mWARNING: You are using pip version 20.2.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install stanfordnlp","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting stanfordnlp\n  Downloading stanfordnlp-0.2.0-py3-none-any.whl (158 kB)\n\u001b[K     |████████████████████████████████| 158 kB 413 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from stanfordnlp) (1.5.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from stanfordnlp) (2.23.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from stanfordnlp) (1.18.5)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from stanfordnlp) (3.12.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from stanfordnlp) (4.45.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.0.0->stanfordnlp) (0.18.2)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->stanfordnlp) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->stanfordnlp) (2020.6.20)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->stanfordnlp) (2.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->stanfordnlp) (1.24.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf->stanfordnlp) (46.1.3.post20200325)\nRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.7/site-packages (from protobuf->stanfordnlp) (1.14.0)\nInstalling collected packages: stanfordnlp\nSuccessfully installed stanfordnlp-0.2.0\n\u001b[33mWARNING: You are using pip version 20.2.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"stanfordnlp.download('en')","execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'stanfordnlp' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-0b88d25c3758>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstanfordnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'stanfordnlp' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install lupyne","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install editdistance","execution_count":6,"outputs":[{"output_type":"stream","text":"Collecting editdistance\n  Downloading editdistance-0.5.3-cp37-cp37m-manylinux1_x86_64.whl (179 kB)\n\u001b[K     |████████████████████████████████| 179 kB 418 kB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: editdistance\nSuccessfully installed editdistance-0.5.3\n\u001b[33mWARNING: You are using pip version 20.2.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install pylucene4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import requests\nimport random\nimport editdistance\nimport stanfordnlp as st\nimport spacy \n\nfrom tqdm import tqdm\ntqdm.pandas()","execution_count":15,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n  from pandas import Panel\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"st.download('en')","execution_count":13,"outputs":[{"output_type":"stream","text":"Using the default treebank \"en_ewt\" for language \"en\".\nWould you like to download the models for: en_ewt now? (Y/n)\nY\n\nDefault download directory: /root/stanfordnlp_resources\nHit enter to continue or type an alternate directory.\n\n\nDownloading models for: en_ewt\nDownload location: /root/stanfordnlp_resources/en_ewt_models.zip\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 235M/235M [00:33<00:00, 7.11MB/s] \n","name":"stderr"},{"output_type":"stream","text":"\nDownload complete.  Models saved to: /root/stanfordnlp_resources/en_ewt_models.zip\nExtracting models file for: en_ewt\nCleaning up...Done.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 1. Importing Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"queries_df = pd.read_csv('../input/ms-marco-queries/msmarco-doctrain-queries.tsv', sep = '\\t', names = ['qid','query'])\nqueries_df = queries_df.set_index('qid')\ndisplay(queries_df.head(10))\ndisplay(queries_df.tail(10))\nprint(len(queries_df.index))","execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"                                                     query\nqid                                                       \n1185869  )what was the immediate impact of the success ...\n1185868  _________ justice is designed to repair the ha...\n1183785                                     elegxo meaning\n645590                      what does physical medicine do\n186154          feeding rice cereal how many times per day\n457407                     most dependable affordable cars\n441383                               lithophile definition\n683408                               what is a flail chest\n484187              put yourself on child support in texas\n666321                      what happens in a wrist sprain","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query</th>\n    </tr>\n    <tr>\n      <th>qid</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1185869</th>\n      <td>)what was the immediate impact of the success ...</td>\n    </tr>\n    <tr>\n      <th>1185868</th>\n      <td>_________ justice is designed to repair the ha...</td>\n    </tr>\n    <tr>\n      <th>1183785</th>\n      <td>elegxo meaning</td>\n    </tr>\n    <tr>\n      <th>645590</th>\n      <td>what does physical medicine do</td>\n    </tr>\n    <tr>\n      <th>186154</th>\n      <td>feeding rice cereal how many times per day</td>\n    </tr>\n    <tr>\n      <th>457407</th>\n      <td>most dependable affordable cars</td>\n    </tr>\n    <tr>\n      <th>441383</th>\n      <td>lithophile definition</td>\n    </tr>\n    <tr>\n      <th>683408</th>\n      <td>what is a flail chest</td>\n    </tr>\n    <tr>\n      <th>484187</th>\n      <td>put yourself on child support in texas</td>\n    </tr>\n    <tr>\n      <th>666321</th>\n      <td>what happens in a wrist sprain</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                                            query\nqid                                              \n234427           how fast should a power chair go\n285891       how many hours do you need for an aa\n1164802                    what causes dry cough?\n87046            causes of irritated mouth tissue\n562255                        what are nephridia?\n19285                  anterolisthesis definition\n558837                     what are fishing flies\n559149   what are fsh levels during perimenopause\n706678                            what is a yowie\n405466                   is carbonic acid soluble","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query</th>\n    </tr>\n    <tr>\n      <th>qid</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>234427</th>\n      <td>how fast should a power chair go</td>\n    </tr>\n    <tr>\n      <th>285891</th>\n      <td>how many hours do you need for an aa</td>\n    </tr>\n    <tr>\n      <th>1164802</th>\n      <td>what causes dry cough?</td>\n    </tr>\n    <tr>\n      <th>87046</th>\n      <td>causes of irritated mouth tissue</td>\n    </tr>\n    <tr>\n      <th>562255</th>\n      <td>what are nephridia?</td>\n    </tr>\n    <tr>\n      <th>19285</th>\n      <td>anterolisthesis definition</td>\n    </tr>\n    <tr>\n      <th>558837</th>\n      <td>what are fishing flies</td>\n    </tr>\n    <tr>\n      <th>559149</th>\n      <td>what are fsh levels during perimenopause</td>\n    </tr>\n    <tr>\n      <th>706678</th>\n      <td>what is a yowie</td>\n    </tr>\n    <tr>\n      <th>405466</th>\n      <td>is carbonic acid soluble</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"stream","text":"367013\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir('./')) # This will print the content of current directory\nprint(os.listdir('../input')) # This will print the content of input directory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Install and import relevant libraries\n!python -m easy_install ../input/compiledlucene/bk/lucene-8.1.1-py3.6-linux-x86_64.egg\n!cp -r ../input/compiledlucene/bk/JCC-3.7-py3.6-linux-x86_64.egg /opt/conda/lib/python3.6/site-packages/\nimport sys\nsys.path\nsys.path.append('/opt/conda/lib/python3.6/site-packages/JCC-3.7-py3.6-linux-x86_64.egg')\nsys.path.append('/opt/conda/lib/python3.6/site-packages/lucene-8.1.1-py3.6-linux-x86_64.egg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lucene","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's first check for any null rows\n\nnull_rows = queries_df[queries_df.isnull().any(axis = 1)]\ndisplay(null_rows)\n\n# Cool, there aren't any","execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Empty DataFrame\nColumns: [query]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query</th>\n    </tr>\n    <tr>\n      <th>qid</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's try to find the average length of a query\n\ndef return_length(n):\n    return len(n)\n\ntotal_len = np.sum(queries_df['query'].apply(return_length))\naverage_query_length = total_len/len(queries_df)\nprint(total_len)\nprint(average_query_length)\n\n# 33 characters seems reasonable.","execution_count":7,"outputs":[{"output_type":"stream","text":"12163703\n33.14243092206543\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average number of word counts in the query\n\ndef number_of_words(n):\n    words = n.split(' ')\n    return len(words)\n\ntotal_word_count = np.sum(queries_df['query'].apply(number_of_words))\naverage_word_count = total_word_count/len(queries_df)\nprint(total_word_count)\nprint(average_word_count)\n\n# An average word count of 6 seems about right for a query","execution_count":8,"outputs":[{"output_type":"stream","text":"2185955\n5.956069676006027\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's now create a corpus of the 2.1M words used in the queries\n\ndef build_vocab(sentences, verbose =  True):\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence.split(' '):\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\nsentences = queries_df['query'].values\n# print(sentences)\nvocab = build_vocab(sentences)\nprint({k: vocab[k] for k in list(vocab)[:5]})\n\n# As we can see, there are some tokens that are misspelled. I'll have to handle that later","execution_count":9,"outputs":[{"output_type":"stream","text":"100%|██████████| 367013/367013 [00:01<00:00, 272426.00it/s]","name":"stderr"},{"output_type":"stream","text":"{')what': 1, 'was': 10493, 'the': 77087, 'immediate': 27, 'impact': 153}\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's now sort the vocab, that way we can remove the mispelled words\n\nsorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n\n# Let's first the see the top 10 most common words in the text, and their counts\nprint(sorted_vocab[:5])\n\n# Let's now have a look at the last 50 terms in the sorted list (they will most probably be mispellings)\nprint(sorted_vocab[-50:])\n\n\n# WOAH!! This is surprising, the last items of the sorted list, aren't actually mispelling, but many of them are just words ending with a question mark or bracket. I am gonna leave them be for now.","execution_count":10,"outputs":[{"output_type":"stream","text":"[('what', 147773), ('is', 133329), ('the', 77087), ('of', 59991), ('a', 55371)]\n[('abbati', 1), ('ridgedale', 1), ('diffuses', 1), ('metts', 1), ('pedipalp', 1), ('synulox', 1), ('pureblood', 1), ('atd', 1), ('westheimer', 1), ('tears?', 1), ('creo', 1), ('air,', 1), ('bulbasaur', 1), ('squirtle', 1), ('thrilling', 1), ('omelettes', 1), ('inherited?', 1), ('(how', 1), ('it)', 1), (\"parent's\", 1), ('electrocuted', 1), ('impinging', 1), ('industry-leading', 1), ('abuja', 1), ('bicubic', 1), ('spenic', 1), ('ex-convicts', 1), ('electroforming', 1), ('buttress?', 1), ('sparely', 1), ('diabetics?', 1), ('448', 1), ('mooring', 1), ('250k', 1), ('exercise/walking', 1), ('pulmonologist?', 1), ('tympanitis', 1), ('rheostat?', 1), ('nauseousness', 1), ('meggitt', 1), ('f81', 1), ('intergenic', 1), ('detasseling', 1), ('julington', 1), ('ciss', 1), ('fantastical', 1), ('filezilla', 1), ('clarisse.', 1), ('nephridia?', 1), ('yowie', 1)]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Evaluation Metrics & Supporting Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def query_sampler(df, percentage_of_samples = 0.9):\n    \"\"\" \n    This function creates a sample set of queries from the orignal dataset\n    \n    Args:\n        df (dataframe) -> The original dataframe\n        percentage_of_samples (float) -> Between 0 and 1\n        \n    Returns:\n        Sampled set of queries.\n    \n    \"\"\"\n    l = len(df)\n    number_of_samples = int(l*percentage_of_samples)\n    print(\"Number of instances being sampled is\", number_of_samples)\n    randomList = random.sample(range(0, l), number_of_samples)\n    return(df.iloc[randomList])\n","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def break_the_query(query_string):\n    \"\"\"\n    This function breaks the query down on character level.\n    \n    Args:\n        query_string(str) -> Original query string\n        \n    Returns:\n        list_of_strings(iterable) -> A list of multiple sub-strings made from the query\n    \"\"\"\n    list_of_strings = []\n    l = len(query_string)\n    for i in range (1, l-1):\n        s = query_string[:i]\n        list_of_strings.append(s)\n    return list_of_strings\n    ","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_test_split(df, train_percentage = 0.9):\n    \"\"\" \n    This function creates a sample set of queries from the orignal dataset\n    \n    Args:\n        df (dataframe) -> The original dataframe\n        train_percentage (float) -> Represents the percentage of training examples. Between 0 and 1\n        \n    Returns:\n        Training and testing data\n    \n    \"\"\"\n    l = len(df)\n    number_of_training_samples = int(l*train_percentage)\n    print(\"Number of instances being sampled for training data is\", number_of_training_samples)\n    train_List = random.sample(range(0, l), number_of_training_samples)\n    test_List = []\n    for i in range(l):\n        if i not in train_List:\n            test_List.append(i)\n    return(df.iloc[train_List], df.iloc[test_List])\n","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def distance_score(str1, str2):\n    \"\"\"\n    This function calculates the Levenshtein distance between the two strings. Will use the library 'editdistance' for this task.\n    \n    Args:\n        str1(string) -> String 1\n        str2(string) -> String 2\n        \n    Returns:\n        d (float) -> Represents the leveshtein distance between the two strings\n    \"\"\"\n    d = editdistance.eval(str1, str2)\n    return d\n\ndef best_distance_on_query(model, query_string):\n    \"\"\"\n    This function returns the best distance score from all the suggestions made for a query.\n    \n    Args:\n        model (object) -> Trained model object \n        query_string (string) -> The query string\n        \n    Returns:\n        Best Levenshtein distance score of all the suggestions made by the model.\n    \"\"\"\n    query_suggestions = model_1.getAutoSuggestions(query_string)\n    least_distance = len(query_string) * 5\n    for query_suggestion in query_suggestions:\n        distance_measure = distance_score(query_suggestion, query_string)\n        if distance_measure < least_distance:\n            least_distance = distance_measure\n    return least_distance","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. AutoSuggest Methods"},{"metadata":{},"cell_type":"markdown","source":"## Part 1: Dictionary Based Approach"},{"metadata":{},"cell_type":"markdown","source":"#### Preparing Data\nIdeally the dictionary-based approach (without analyzers) shouldn't include words in the build dictionary, but I am gonna try it out as well. \nSo, the data would be prepared for TRIE in two batches:\n1. Simple queries data.\n2. Simple queries data, along with all the words used in that queries data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This cell contains the point 1 of the two data forms described above.\n\n#First I am gonna create a sample of ~36000 queries\nqueries_df_1 = query_sampler(queries_df, percentage_of_samples = 0.1)\n\n# Next up, I'll remove the qid from the index\nqueries_df_1.reset_index(drop = True, inplace = True)\n\n# Next I'll divide the queries data into train and test set. With 95% training data\ntrain_queries_df_1, test_queries_df_1 = train_test_split(queries_df_1, train_percentage = 0.95)\ntrain_queries_data_1 = train_queries_df_1['query'].values\ntest_queries_data_1 = test_queries_df_1['query'].values\nprint(train_queries_data_1)\nprint(len(train_queries_data_1))\n#print(queries_data_1)\n#print(len(queries_data_1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This cell contains data created by definition of point 2 of the two data forms described above.\n\n#First I am gonna create a sample of ~36000 queries\nqueries_df_2 = query_sampler(queries_df, percentage_of_samples = 0.1)\n\n# Next up, I'll remove the qid from the index\nqueries_df_2.reset_index(drop = True, inplace = True)\n\n# Next up I am gonna add all the words in these queries seperately as well\nsentences = queries_df_2['query'].values\ntokens = []\nfor sentence in tqdm(sentences):\n    for word in sentence.split(' '):\n        # I will also have to remove the special characters from the words before putting them into the vocab. I will not remove numerics because many search queries might require numbers like WC20 or XL5 etc\n        token = \"\"\n        for character in word:\n            if character.isalnum():\n                token += character\n        tokens.append(token)\nvocab = list(set(tokens))\nfor word in vocab:\n    queries_df_2 = queries_df_2.append({'query' : word}, ignore_index=True)\n    \ndisplay(queries_df_2.tail(10))\ndisplay(len(queries_df_2))\n\n# Next I'll divide the queries data into train and test set. With 95% training data\ntrain_queries_df_2, test_queries_df_2 = train_test_split(queries_df_2, train_percentage = 0.95)\ntrain_queries_data_2 = train_queries_df_2['query'].values\ntest_queries_data_2 = test_queries_df_2['query'].values\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Creation: TRIE"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrieNode(): \n    def __init__(self): \n          \n        # Initialising one node for trie \n        self.children = {} \n        self.last = False\n  \n\nclass Trie(): \n    def __init__(self): \n          \n        # Initialising the trie structure. \n        self.root = TrieNode() \n        self.word_list = []\n        self.count = 0\n  \n    def formTrie(self, keys): \n          \n        # Forms a trie structure with the given set of strings \n        # if it does not exists already else it merges the key \n        # into it by extending the structure as required \n        for key in keys: \n            self.insert(key) # inserting one key to the trie. \n  \n    def insert(self, key): \n          \n        # Inserts a key into trie if it does not exist already. \n        # And if the key is a prefix of the trie node, just  \n        # marks it as leaf node. \n        node = self.root \n  \n        for a in list(key): \n            if not node.children.get(a): \n                node.children[a] = TrieNode() \n  \n            node = node.children[a] \n  \n        node.last = True\n  \n    def search(self, key): \n          \n        # Searches the given key in trie for a full match \n        # and returns True on success else returns False. \n        node = self.root \n        found = True\n  \n        for a in list(key): \n            if not node.children.get(a): \n                found = False\n                break\n  \n            node = node.children[a] \n  \n        return node and node.last and found \n  \n    def suggestionsRec(self, node, word): \n          \n        # Method to recursively traverse the trie \n        # and return a whole word.  \n        if node.last: \n            self.word_list.append(word) \n  \n        for a,n in node.children.items(): \n            self.suggestionsRec(n, word + a)\n            \n    def limitedSuggestionsRec(self, node, word, no_of_suggestions): \n          \n        # Method to recursively traverse the trie \n        # and return a whole word. But limit the number of routes ventured to no_of_suggestions\n        if node.last:\n            self.count = self.count + 1\n            if self.count <= no_of_suggestions:\n                self.word_list.append(word)\n\n        for a,n in node.children.items():\n            self.suggestionsRec(n, word + a) \n            \n    \n    def printAutoSuggestions(self, key, no_of_suggestions = 0): \n        # Returns all the words in the trie whose common \n        # prefix is the given key thus listing out all  \n        # the suggestions for autocomplete. \n        node = self.root \n        not_found = False\n        temp_word = '' \n        print('This function was called')\n        characters = list(key)\n        for a in characters:\n            if not node.children.get(a): \n                not_found = True\n                break\n  \n            temp_word += a \n            node = node.children[a] \n  \n        if not_found: \n            return 0\n        elif node.last and not node.children: \n            return -1\n  \n        if no_of_suggestions>0:\n            self.count = 0\n            self.limitedSuggestionsRec(node, temp_word, no_of_suggestions)\n        else:\n            self.suggestionsRec(node, temp_word)\n  \n        for s in self.word_list: \n            print(s) \n        return 1\n    \n    def getAutoSuggestions(self, key, no_of_suggestions = 0): \n          \n        # Returns all the words in the trie whose common \n        # prefix is the given key thus listing out all  \n        # the suggestions for autocomplete. \n        node = self.root \n        not_found = False\n        temp_word = '' \n  \n        characters = list(key)\n        for a in characters:\n            if not node.children.get(a): \n                not_found = True\n                break\n  \n            temp_word += a \n            node = node.children[a] \n  \n        if not_found: \n            return 0\n        elif node.last and not node.children: \n            return -1\n        \n        if no_of_suggestions>0:\n            self.count = 0\n            self.limitedSuggestionsRec(node, temp_word, no_of_suggestions)\n        else:\n            self.suggestionsRec(node, temp_word)\n   \n        return self.word_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Training\nWe are gonna create two TRIE structures (one for each kind of data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1 = Trie()\nmodel_1.formTrie(train_queries_data_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_2 = Trie()\nmodel_2.formTrie(train_queries_data_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Testing\nWe are gonna test each of the two models trained above using 3 metrics of evaluation:\n1. Intuition Based (Will analyze the relevance of the search suggestions manually)\n2. Distance Based (Will produce the overall score of test set using Levenshtein Distance)\n3. Relevance Based (Will use semantic textual similarity to determine relevance of each of the suggestions)."},{"metadata":{},"cell_type":"markdown","source":"##### Model 1:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I am first gonna define the three strings I'll be using for Intuition based testing\nstring1 = 'What building method might use a balloon frame?'\nstring2 = 'What is causing rash on arms'\nstring3 = 'What causes your glands on the top of your throat to swell'\n\n# Next up, I am gonna create a list of sub strings from these queries\nlist_string1 = break_the_query(string1)\nlist_string2 = break_the_query(string2)\nlist_string3 = break_the_query(string3)\n\n# Now, I'll print the query results after each new character being typed in\ncomp = model_1.printAutoSuggestions(string1) \nif comp == -1: \n    print(\"No other strings found with this prefix\\n\") \nelif comp == 0: \n    print(\"No string found with this prefix\\n\") \nbreak\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 2: Analyzer Based Approach"},{"metadata":{},"cell_type":"markdown","source":"#### Data Preparation\nI am gonna prepare the data with the following steps:\n1. Tokenize the data\n2. Create a Dictionary of Words, and their contexts."},{"metadata":{"trusted":true},"cell_type":"code","source":"#First I am gonna create a sample of ~36000 queries\nqueries_df_3 = query_sampler(queries_df, percentage_of_samples = 0.1)\n\n# Next up, I'll remove the qid from the index\nqueries_df_3.reset_index(drop = True, inplace = True)\ndisplay(queries_df_3.head(10))","execution_count":15,"outputs":[{"output_type":"stream","text":"Number of instances being sampled is 36701\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"                                               query\n0               what musician was known as slow hand\n1  how much does a lawyer charge to help me file ...\n2                          how big is a playing card\n3                                why do echoes occur\n4  how much education is need to become a pharmacist\n5  health maintenance organization is what type o...\n6                                    wier definition\n7                what is adverse selection economics\n8                                irs id verification\n9                           gingivoplasty definition","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>what musician was known as slow hand</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>how much does a lawyer charge to help me file ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>how big is a playing card</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>why do echoes occur</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>how much education is need to become a pharmacist</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>health maintenance organization is what type o...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>wier definition</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>what is adverse selection economics</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>irs id verification</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>gingivoplasty definition</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The problem with this data is that it cannot be used for NER tasks as it is, that is, it is not capitalized. And therefore, will not be able to identify NER tags properly.\n# To resolve this problem, we will perform manual True Casing by using POS tagger of StanfordNLP, and then the results thus produced would be fed into SpaCy NER model.\n\nstf_nlp = st.Pipeline(processors='tokenize,mwt,pos')\nspacy_nlp = spacy.load('en_core_web_sm')","execution_count":16,"outputs":[{"output_type":"stream","text":"Use device: cpu\n---\nLoading: tokenize\nWith settings: \n{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n---\nLoading: pos\nWith settings: \n{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\nDone loading processors!\n---\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Analyzer:\n    \"\"\"\n    This class performs several analysis on the query data (NER, POS, etc.) and creates a word dictionary of a form appropriate for usage in fast-autocomplete library\n    \n    Args:\n        df (DataFrame) -> Contains all the queries\n        \n    Returns:\n        words (dictionary) -> Dictionary of the fast-autocomplete prescribed format\n    \n    \"\"\"\n    def __init__(self, df):\n        self.words = {}\n        self.df = df\n        \n        \n    def get_entities(self, query_string):\n        '''\n        This function takes in the query string and returns all the Named Entities present in the caseless string being passed to it.\n        \n        Args:\n            query_string (string) -> A single query of the user\n            \n        Returns:\n            ner_words (list) -> Contains all the identified NER terms in the text, along with their corresponding tag and start and end points.\n        \n        '''\n        # Here, I'll first perform Truecasing using StanfordNLP, and then use NER model from Spacy\n        doc = stf_nlp(query_string)\n        truecased_list = [w.text.capitalize() if w.upos in [\"PROPN\",\"NNS\"] else w.text for sent in doc.sentences for w in sent.words]\n        truecased_query = ''\n        for words in truecased_list:\n            truecased_query.append(word)\n            truecased_query.append(' ')\n        \n        \n        \n    def add_word_to_dictionary(self, current_word, prior1_word = None, prior2_word = None, ner_context = None):\n        '''\n        This function adds a new word/phraze to the words dictionary\n        \n        Args:\n            current_word (string) -> A token (word/phraze) to be added to the dictionary\n            ner_context (string) -> Describes the type of entity, if there is one\n            prior1_word (string) -> The word immediately prior to the current one\n            prior2_word (string) -> The word before the prior1_word\n        \n        '''\n        self.words[current_word] = []              # Initializing the word in words dictionary\n        self.words[current_word][0] = {}           # Initializing the Context dictionary for that word\n        self.words[current_word][1] = current_word         # Setting the display value equal to the word string\n        self.words[current_word][2] = 1            # Initializing the Count to be equal to zero\n        if prior1_word != None:\n            self.words[current_word][0][\"prior1\"] = prior1_word\n        if prior2_word != None:\n            self.words[current_word][0][\"prior2\"] = prior2_word\n        # Write the code for adding the NER value to the context of each current_word\n        if ner_context != None:\n            self.words[current_word][0][\"type\"] = ner_context \n        # Write the code for adding a POS tag value to the context of each current_word. Will have to decide whether or not I want to do this.\n     \n    \n    \n    def get_words_dictionary(self, context_size = 1):\n        \"\"\"\n        This function creates a dictionary of words in the format that can be directly fed into fast-autocomplete model.\n\n        Args:\n            context_size (int) -> The size of the context window to be turned into the dictionary format.\n        \"\"\"\n        for i in range(len(self.df)):\n            query = self.df['query'][i]\n            ner_words = get_entities(query_string = query)\n            query_words = query.split(' ')\n            \n\n\n\n\n\n\n\n# This function would create the dictionary of words along with their contexts\n\ndef create_words_dictionary(df, context_size = 1):\n    \"\"\"\n    This function creates a dictionary of words in the format that can be directly fed into fast-autocomplete model.\n    \n    Args:\n        df (DataFrame) -> Contains all the queries\n        context_size (int) -> The size of the context window to be turned into the dictionary format.\n    \"\"\"\n    words = {}\n    \n    for i in range(len(df)):\n        query = df['query'][i]\n        ner_words = get_entities(query)             #gets the entities from the NER model\n        query_words = query.split(' ')\n        for j in range(len(query_words)):\n            current_word = query_words[j]\n            if current_word not in words:\n                words[current_word] = []              # Initializing the word in words dictionary\n                words[current_word][0] = {}           # Initializing the Context dictionary for that word\n                words[current_word][1] = current_word         # Setting the display value equal to the word string\n                words[current_word][2] = 1            # Initializing the Count to be equal to zero\n                if (j-1) >= 0:\n                    words[current_word][0][\"prior1\"] = query_words[j-1]\n                if (j-2) >= 0:\n                    words[current_word][0][\"prior2\"] = query_words[j-2]\n                # Write the code for adding the NER value to the context of each current_word\n                \n                \n                # Write the code for adding a POS tag value to the context of each current_word. Will have to decide whether or not I want to do this.\n                \n            else:\n                context_of_existing_current_word = words[current_word][0]\n                existing_context_list = context_of_existing_current_word.keys()\n                new_word_context = {}\n                if (j-1) >= 0:\n                    new_word_context[\"prior1\"] = query_words[j-1]\n                if (j-2) >= 0:\n                    new_word_context[\"prior2\"] = query_words[j-2]\n                # Writ the code for adding NER value to \"new_word_context\"\n                \n                \n                \n                # Write the code for adding POS tags to the \"new_word_context\"\n                \n                \n                # Following code checks whether the word has been used in the same context or not\n                same = True\n                for context in existing_context_list:\n                    if context in new_word_context.keys():\n                        if new_word_context[context] == context_of_existing_current_word[context]:\n                            same = True\n                        else:\n                            same = False\n                            break\n                if same == True:\n                    words[current_word][2] += 1           # Increasing the count of the word by one\n                else:\n                    \n                \n            \n        \n\nwords = {}\n\nfor line in csv_gen:\n    make = line['make']\n    model = line['model']\n    if make != model:\n        local_words = [model, '{} {}'.format(make, model)]\n        while local_words:\n            word = local_words.pop()\n            if word not in words:\n                words[word] = {}\n    if make not in words:\n        words[make] = {}\nreturn words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 3: Content Based Approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}